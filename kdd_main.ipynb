{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7o0kZoi5Ubf-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set = './data/KDDTrain+.txt'\n",
    "#test_set = 'KDDTest+.txt'\n",
    "\n",
    "train_df = pd.read_csv(train_set, header=None)\n",
    "#test_df = pd.read_csv(test_set, header=None)\n",
    "\n",
    "train_data = train_df.iloc[:, :-1]\n",
    "#test_data = test_df.iloc[:, :-1]\n",
    "\n",
    "#replace not 'normal' labels with 'attack'\n",
    "train_data.iloc[:, -1] = np.where(train_data.iloc[:, -1] != 'normal', 'attack', train_data.iloc[:, -1])\n",
    "#test_data.iloc[:, -1] = np.where(test_data.iloc[:, -1] != 'normal', 'attack', test_data.iloc[:, -1])\n",
    "\n",
    "#explicitly assigning numerical labels to 'attack' and 'normal'\n",
    "train_data.iloc[:, -1] = np.where(train_data.iloc[:, -1] != 'normal', 1, 0)\n",
    "#test_data.iloc[:, -1] = np.where(test_data.iloc[:, -1] != 'normal', 1, 0)\n",
    "\n",
    "for col in train_data.columns[train_data.dtypes == object]:\n",
    "    encoder = LabelEncoder()\n",
    "    train_data[col] = encoder.fit_transform(train_data[col])\n",
    "    #test_data[col] = encoder.transform(test_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vHq22rg4odoS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(dataset)\n",
    "        self.indices = list(range(self.num_samples))\n",
    "\n",
    "        #indices for each class\n",
    "        self.positive_indices = [i for i in range(self.num_samples) if self.dataset[i][1] == 1]\n",
    "        self.negative_indices = [i for i in range(self.num_samples) if self.dataset[i][1] == 0]\n",
    "\n",
    "        #number of samples for each class in each batch\n",
    "        self.num_positive_samples = (batch_size + 1) // 2\n",
    "        self.num_negative_samples = batch_size - self.num_positive_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        #shuffle\n",
    "        np.random.shuffle(self.positive_indices)\n",
    "        np.random.shuffle(self.negative_indices)\n",
    "        for i in range(0, self.num_samples, self.batch_size):\n",
    "            positive_batch_indices = self.positive_indices[i:i+self.num_positive_samples]\n",
    "            negative_batch_indices = self.negative_indices[i:i+self.num_negative_samples]\n",
    "\n",
    "            batch_indices = positive_batch_indices + negative_batch_indices\n",
    "            if len(batch_indices) < self.batch_size:\n",
    "                num_missing_samples = self.batch_size - len(batch_indices)\n",
    "                additional_negative_indices = self.negative_indices[:num_missing_samples]\n",
    "                batch_indices += additional_negative_indices\n",
    "\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "#splitting datasets\n",
    "X_train, y_train = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
    "#X_test, y_test = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
    "\n",
    "#normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "test_size=0.2\n",
    "val_split=0.2\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(X_train_scaled, y_train, test_size=test_size, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split/(1-test_size), random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train.values).long())\n",
    "val_dataset = TensorDataset(torch.tensor(X_val).float(), torch.tensor(y_val.values).long())\n",
    "test_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test.values).long())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_balanced_sampler = BalancedBatchSampler(train_dataset, batch_size)\n",
    "val_balanced_sampler = BalancedBatchSampler(val_dataset, batch_size)\n",
    "test_balanced_sampler = BalancedBatchSampler(test_dataset, batch_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_balanced_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_balanced_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_sampler=test_balanced_sampler)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EKQ-L_6KpCFW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 75583\n",
      "Size of test dataset: 25195\n",
      "Batch size: 64\n",
      "Number of positive samples in train dataset: 35128\n",
      "Number of positive samples in test dataset: 11773\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train dataset:\", len(train_loader.dataset))\n",
    "print(\"Size of test dataset:\", len(test_loader.dataset))\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "num_positives_train = sum(1 for _, label in train_dataset if label == 1)\n",
    "num_positives_test = sum(1 for _, label in test_dataset if label == 1)\n",
    "\n",
    "print(\"Number of positive samples in train dataset:\", num_positives_train)\n",
    "print(\"Number of positive samples in test dataset:\", num_positives_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "LNzidFLz-UxI",
    "outputId": "7866a328-2009-4f5f-baea-49ebc4f7cca3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = './data/KDDTrain+.txt'\n",
    "column_names = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\",\n",
    "    \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n",
    "    \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "    \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\",\n",
    "    \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\",\n",
    "    \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"\n",
    "]\n",
    "train_df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "#train_numeric = train_df.drop(columns=[\"protocol_type\", \"service\", \"flag\", \"label\"])\n",
    "#sns.pairplot(train_numeric.sample(frac=0.01), diag_kind='kde')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "diLqBj_TVsT0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1181], Loss: 0.0828\n",
      "Epoch [1/5], Step [200/1181], Loss: 0.0994\n",
      "Epoch [1/5], Step [300/1181], Loss: 0.0919\n",
      "Epoch [1/5], Step [400/1181], Loss: 0.0908\n",
      "Epoch [1/5], Step [500/1181], Loss: 0.0998\n",
      "Epoch [1/5], Step [600/1181], Loss: 0.0003\n",
      "Epoch [1/5], Step [700/1181], Loss: 0.0001\n",
      "Epoch [1/5], Step [800/1181], Loss: 0.0000\n",
      "Epoch [1/5], Step [900/1181], Loss: 0.0000\n",
      "Epoch [1/5], Step [1000/1181], Loss: 0.0001\n",
      "Epoch [1/5], Step [1100/1181], Loss: 0.0000\n",
      "Validation Accuracy after Epoch 1: 94.93 %\n",
      "Epoch [2/5], Step [100/1181], Loss: 0.0246\n",
      "Epoch [2/5], Step [200/1181], Loss: 0.0603\n",
      "Epoch [2/5], Step [300/1181], Loss: 0.0769\n",
      "Epoch [2/5], Step [400/1181], Loss: 0.0222\n",
      "Epoch [2/5], Step [500/1181], Loss: 0.0394\n",
      "Epoch [2/5], Step [600/1181], Loss: 0.0004\n",
      "Epoch [2/5], Step [700/1181], Loss: 0.0000\n",
      "Epoch [2/5], Step [800/1181], Loss: 0.0001\n",
      "Epoch [2/5], Step [900/1181], Loss: 0.0000\n",
      "Epoch [2/5], Step [1000/1181], Loss: 0.0000\n",
      "Epoch [2/5], Step [1100/1181], Loss: 0.0000\n",
      "Validation Accuracy after Epoch 2: 76.71 %\n",
      "Epoch [3/5], Step [100/1181], Loss: 0.1005\n",
      "Epoch [3/5], Step [200/1181], Loss: 0.0628\n",
      "Epoch [3/5], Step [300/1181], Loss: 0.1821\n",
      "Epoch [3/5], Step [400/1181], Loss: 0.0140\n",
      "Epoch [3/5], Step [500/1181], Loss: 0.1152\n",
      "Epoch [3/5], Step [600/1181], Loss: 0.0015\n",
      "Epoch [3/5], Step [700/1181], Loss: 0.0003\n",
      "Epoch [3/5], Step [800/1181], Loss: 0.0000\n",
      "Epoch [3/5], Step [900/1181], Loss: 0.0000\n",
      "Epoch [3/5], Step [1000/1181], Loss: 0.0001\n",
      "Epoch [3/5], Step [1100/1181], Loss: 0.0001\n",
      "Validation Accuracy after Epoch 3: 98.00 %\n",
      "Epoch [4/5], Step [100/1181], Loss: 0.0094\n",
      "Epoch [4/5], Step [200/1181], Loss: 0.4291\n",
      "Epoch [4/5], Step [300/1181], Loss: 0.0430\n",
      "Epoch [4/5], Step [400/1181], Loss: 0.0143\n",
      "Epoch [4/5], Step [500/1181], Loss: 0.0021\n",
      "Epoch [4/5], Step [600/1181], Loss: 0.0001\n",
      "Epoch [4/5], Step [700/1181], Loss: 0.0002\n",
      "Epoch [4/5], Step [800/1181], Loss: 0.0000\n",
      "Epoch [4/5], Step [900/1181], Loss: 0.0000\n",
      "Epoch [4/5], Step [1000/1181], Loss: 0.0000\n",
      "Epoch [4/5], Step [1100/1181], Loss: 0.0000\n",
      "Validation Accuracy after Epoch 4: 98.06 %\n",
      "Epoch [5/5], Step [100/1181], Loss: 0.0430\n",
      "Epoch [5/5], Step [200/1181], Loss: 0.0142\n",
      "Epoch [5/5], Step [300/1181], Loss: 0.0592\n",
      "Epoch [5/5], Step [400/1181], Loss: 0.0380\n",
      "Epoch [5/5], Step [500/1181], Loss: 0.0106\n",
      "Epoch [5/5], Step [600/1181], Loss: 0.0003\n",
      "Epoch [5/5], Step [700/1181], Loss: 0.0001\n",
      "Epoch [5/5], Step [800/1181], Loss: 0.0000\n",
      "Epoch [5/5], Step [900/1181], Loss: 0.0001\n",
      "Epoch [5/5], Step [1000/1181], Loss: 0.0000\n",
      "Epoch [5/5], Step [1100/1181], Loss: 0.0000\n",
      "Validation Accuracy after Epoch 5: 76.75 %\n",
      "Time taken for testing: 0.48 seconds\n",
      "Accuracy of the FFNN on the test dataset: 76.68 %\n",
      "Precision of the FFNN on the test dataset: 1.00\n",
      "True Positives (TP): 8\n",
      "True Negatives (TN): 19328\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 5880\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        #hidden layers\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc8 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc8(out)\n",
    "        return out\n",
    "\n",
    "def train_ffnn(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print('Validation Accuracy after Epoch {}: {:.2f} %'.format(epoch+1, val_accuracy))\n",
    "\n",
    "from sklearn.metrics import precision_score, confusion_matrix\n",
    "def test_ffnn(model, test_loader):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(labels.tolist())\n",
    "        end_time = time.time()\n",
    "        accuracy = 100 * correct / total\n",
    "        precision = precision_score(targets, predictions)\n",
    "        tn, fp, fn, tp = confusion_matrix(targets, predictions).ravel()\n",
    "        print('Time taken for testing: {:.2f} seconds'.format(end_time - start_time))\n",
    "        print('Accuracy of the FFNN on the test dataset: {:.2f} %'.format(accuracy))\n",
    "        print('Precision of the FFNN on the test dataset: {:.2f}'.format(precision))\n",
    "        print('True Positives (TP):', tp)\n",
    "        print('True Negatives (TN):', tn)\n",
    "        print('False Positives (FP):', fp)\n",
    "        print('False Negatives (FN):', fn)\n",
    "\n",
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "model = FFNN(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "train_ffnn(model, criterion, optimizer, train_loader,val_loader, num_epochs)\n",
    "test_ffnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QBPPxtShabV3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1181], Loss: 0.1313\n",
      "Epoch [1/5], Step [200/1181], Loss: 0.2287\n",
      "Epoch [1/5], Step [300/1181], Loss: 0.2420\n",
      "Epoch [1/5], Step [400/1181], Loss: 0.0586\n",
      "Epoch [1/5], Step [500/1181], Loss: 0.0768\n",
      "Epoch [1/5], Step [600/1181], Loss: 0.0163\n",
      "Epoch [1/5], Step [700/1181], Loss: 0.0601\n",
      "Epoch [1/5], Step [800/1181], Loss: 0.0313\n",
      "Epoch [1/5], Step [900/1181], Loss: 0.0531\n",
      "Epoch [1/5], Step [1000/1181], Loss: 0.0599\n",
      "Epoch [1/5], Step [1100/1181], Loss: 0.0175\n",
      "Epoch [2/5], Step [100/1181], Loss: 0.0188\n",
      "Epoch [2/5], Step [200/1181], Loss: 0.0372\n",
      "Epoch [2/5], Step [300/1181], Loss: 0.0233\n",
      "Epoch [2/5], Step [400/1181], Loss: 0.0128\n",
      "Epoch [2/5], Step [500/1181], Loss: 0.0118\n",
      "Epoch [2/5], Step [600/1181], Loss: 0.0153\n",
      "Epoch [2/5], Step [700/1181], Loss: 0.0522\n",
      "Epoch [2/5], Step [800/1181], Loss: 0.0263\n",
      "Epoch [2/5], Step [900/1181], Loss: 0.0534\n",
      "Epoch [2/5], Step [1000/1181], Loss: 0.0276\n",
      "Epoch [2/5], Step [1100/1181], Loss: 0.0218\n",
      "Epoch [3/5], Step [100/1181], Loss: 0.0620\n",
      "Epoch [3/5], Step [200/1181], Loss: 0.0115\n",
      "Epoch [3/5], Step [300/1181], Loss: 0.0145\n",
      "Epoch [3/5], Step [400/1181], Loss: 0.0025\n",
      "Epoch [3/5], Step [500/1181], Loss: 0.0032\n",
      "Epoch [3/5], Step [600/1181], Loss: 0.0229\n",
      "Epoch [3/5], Step [700/1181], Loss: 0.0260\n",
      "Epoch [3/5], Step [800/1181], Loss: 0.0077\n",
      "Epoch [3/5], Step [900/1181], Loss: 0.0067\n",
      "Epoch [3/5], Step [1000/1181], Loss: 0.0993\n",
      "Epoch [3/5], Step [1100/1181], Loss: 0.0159\n",
      "Epoch [4/5], Step [100/1181], Loss: 0.0080\n",
      "Epoch [4/5], Step [200/1181], Loss: 0.0096\n",
      "Epoch [4/5], Step [300/1181], Loss: 0.0039\n",
      "Epoch [4/5], Step [400/1181], Loss: 0.0049\n",
      "Epoch [4/5], Step [500/1181], Loss: 0.0166\n",
      "Epoch [4/5], Step [600/1181], Loss: 0.0024\n",
      "Epoch [4/5], Step [700/1181], Loss: 0.0055\n",
      "Epoch [4/5], Step [800/1181], Loss: 0.0435\n",
      "Epoch [4/5], Step [900/1181], Loss: 0.0058\n",
      "Epoch [4/5], Step [1000/1181], Loss: 0.0055\n",
      "Epoch [4/5], Step [1100/1181], Loss: 0.0011\n",
      "Epoch [5/5], Step [100/1181], Loss: 0.0044\n",
      "Epoch [5/5], Step [200/1181], Loss: 0.0007\n",
      "Epoch [5/5], Step [300/1181], Loss: 0.0072\n",
      "Epoch [5/5], Step [400/1181], Loss: 0.0064\n",
      "Epoch [5/5], Step [500/1181], Loss: 0.0114\n",
      "Epoch [5/5], Step [600/1181], Loss: 0.0130\n",
      "Epoch [5/5], Step [700/1181], Loss: 0.0046\n",
      "Epoch [5/5], Step [800/1181], Loss: 0.0009\n",
      "Epoch [5/5], Step [900/1181], Loss: 0.0083\n",
      "Epoch [5/5], Step [1000/1181], Loss: 0.0039\n",
      "Epoch [5/5], Step [1100/1181], Loss: 0.0710\n",
      "Time taken for testing: 0.54 seconds\n",
      "Accuracy of the CNN on the test dataset: 99.31 %\n",
      "Precision of the CNN on the test dataset: 0.99\n",
      "True Positives (TP): 11676\n",
      "True Negatives (TN): 13344\n",
      "False Positives (FP): 78\n",
      "False Negatives (FN): 97\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_out_size = self._get_conv_out_size(input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_out_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def _get_conv_out_size(self, input_shape):\n",
    "        batch_size = 1\n",
    "        input_tensor = torch.autograd.Variable(torch.rand(batch_size, *input_shape))\n",
    "        output_feat = self._forward_features(input_tensor)\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        return out\n",
    "\n",
    "def train_cnn(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "from sklearn.metrics import precision_score, confusion_matrix\n",
    "\n",
    "def test_cnn(model, test_loader):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(labels.tolist())\n",
    "        end_time = time.time()\n",
    "        accuracy = 100 * correct / total\n",
    "        precision = precision_score(targets, predictions)\n",
    "        tn, fp, fn, tp = confusion_matrix(targets, predictions).ravel()\n",
    "\n",
    "        print('Time taken for testing: {:.2f} seconds'.format(end_time - start_time))\n",
    "        print('Accuracy of the CNN on the test dataset: {:.2f} %'.format(accuracy))\n",
    "        print('Precision of the CNN on the test dataset: {:.2f}'.format(precision))\n",
    "        print('True Positives (TP):', tp)\n",
    "        print('True Negatives (TN):', tn)\n",
    "        print('False Positives (FP):', fp)\n",
    "        print('False Negatives (FN):', fn)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train).float()\n",
    "y_train_tensor = torch.tensor(y_train.values).long()\n",
    "X_val_tensor = torch.tensor(X_val).float()\n",
    "y_val_tensor = torch.tensor(y_val.values).long()\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_test_tensor = torch.tensor(y_test.values).long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "input_shape = (X_train.shape[1],)\n",
    "num_classes = 2\n",
    "model = CNN(input_shape, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "train_cnn(model, criterion, optimizer, train_loader, num_epochs)\n",
    "\n",
    "test_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
